# Machine Learning Basics Repository

This repository contains code and resources that I used to learn the basics of machine learning. Each folder represents a different machine learning concept or algorithm, along with examples and implementations. Below is a brief description of each topic covered:

## Table of Contents

1. [Linear Regression (Single Variable)](#linear-regression-single-variable)
2. [Linear Regression (Multivariate)](#linear-regression-multivariate)
3. [Logistic Regression (Binary)](#logistic-regression-binary)
4. [Logistic Regression (Multiclass)](#logistic-regression-multiclass)
5. [Decision Trees](#decision-trees)
6. [Random Forest](#random-forest)
7. [Support Vector Machine (SVM)](#support-vector-machine-svm)
8. [K-Means Clustering](#k-means-clustering)
9. [Naive Bayes](#naive-bayes)
10. [One Hot Encoding](#one-hot-encoding)
11. [Feature Engineering](#feature-engineering)
12. [K-Fold Cross-Validation](#k-fold-cross-validation)
13. [Train-Test Split](#train-test-split)
14. [Saving the Model](#saving-the-model)

---

### Linear Regression (Single Variable)

Implementation of linear regression using a single feature to predict a target value.

### Linear Regression (Multivariate)

A more advanced linear regression model where multiple features are used to predict the target variable.

### Logistic Regression (Binary)

Logistic regression for binary classification tasks, such as predicting true/false or yes/no outcomes.

### Logistic Regression (Multiclass)

Logistic regression adapted for multiclass classification problems.

### Decision Trees

Implementation of decision trees for both classification and regression tasks.

### Random Forest

Ensemble learning technique using multiple decision trees to improve model accuracy and prevent overfitting.

### Support Vector Machine (SVM)

Implementation of SVM for classification problems, with examples of linear and non-linear decision boundaries.

### K-Means Clustering

Unsupervised learning algorithm for clustering data points into a specified number of groups (k).

### Naive Bayes

Probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between features.

### One Hot Encoding

Encoding categorical variables into binary vectors, used as a preprocessing step for machine learning algorithms.

### Feature Engineering

Techniques to create new features or modify existing features to improve model performance.

### K-Fold Cross-Validation

A method of evaluating machine learning models by splitting the data into k subsets and performing k training/testing cycles.

### Train-Test Split

Splitting the dataset into a training set and a testing set to evaluate model performance.

### Saving the Model

Examples of saving trained models for future use with libraries like `joblib` and `pickle`.

---

## How to Use

To explore the different machine learning techniques, navigate to the respective folders. Each folder contains Python code and examples explaining the concept. The code can be executed in a Jupyter Notebook or any Python environment.

## Requirements

- Python 3.x
- Libraries: `numpy`, `pandas`, `scikit-learn`, `matplotlib`, `seaborn`
- Install dependencies by running: 
  ```bash
  pip install -r requirements.txt
